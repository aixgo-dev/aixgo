---
title: "Aixgo v0.2.0: Production-Grade VertexAI, Enhanced Security, and Stability"
date: 2025-12-26
description: "Aixgo v0.2.0 brings major improvements to the VertexAI provider with Google Gen AI SDK migration, production hardening fixes, enhanced security with SSRF protection, and streamlined CI/CD workflows."
tags: ["release", "vertexai", "security", "production", "google-cloud"]
author: "Aixgo Team"
---

We're excited to announce **Aixgo v0.2.0**, a release focused on production readiness, security hardening, and developer experience improvements. This release brings a major upgrade to our VertexAI provider, critical stability fixes, and enhanced security features.

## What's New in v0.2.0

### VertexAI Provider: Migration to Google Gen AI SDK

The biggest change in v0.2.0 is the complete migration of our VertexAI provider from manual HTTP API calls to the official [Google Generative AI SDK](https://pkg.go.dev/google.golang.org/genai) (`v0.5.0`). This brings significant improvements in maintainability, authentication, and long-term SDK support.

#### Key Benefits

**Simplified Authentication**

Previously, authentication required manual OAuth2 token management. Now, the provider uses Application Default Credentials (ADC), automatically discovering credentials from:

- Service account key files via `GOOGLE_APPLICATION_CREDENTIALS`
- Google Cloud SDK credentials
- Compute Engine/Cloud Run metadata service

```go
// Before: Manual token management (removed)
// Now: Automatic ADC-based authentication
provider, err := provider.NewVertexAIProvider("gemini-1.5-pro", "your-project-id")
if err != nil {
    log.Fatal(err)
}
defer provider.Close() // New: Graceful cleanup
```

**Better SDK Support**

By using the official SDK, we gain:

- Automatic handling of API versioning
- Built-in retry logic and error handling
- Support for new features as Google releases them
- Reduced maintenance burden

**Configuration Example**

```yaml
agents:
  - name: analyzer
    role: react
    model: gemini-1.5-pro
    prompt: |
      You are a data analyst specializing in time-series analysis.
      Analyze the provided data and identify trends.
    tools:
      - name: query_database
        description: Query the analytics database
```

```bash
# Set your GCP project ID
export GOOGLE_CLOUD_PROJECT=your-project-id

# Configure ADC (one of the following):
# Option 1: Service account key file
export GOOGLE_APPLICATION_CREDENTIALS=/path/to/service-account-key.json

# Option 2: Use gcloud CLI
gcloud auth application-default login

# Run your agent
go run main.go
```

### Production Hardening

v0.2.0 includes critical fixes for production deployments that improve reliability and prevent resource leaks.

#### Fixed Goroutine Leak in Streaming

**Problem**: Streaming responses could leave goroutines running indefinitely if the context was cancelled or an error occurred.

**Solution**: Added context-aware cleanup ensuring goroutines terminate properly when:

- Client cancels the request
- Stream encounters an error
- Normal completion occurs

```go
// Streaming now properly cleans up resources
ctx, cancel := context.WithTimeout(context.Background(), 30*time.Second)
defer cancel()

stream, err := provider.CompleteStream(ctx, request)
if err != nil {
    log.Fatal(err)
}

for chunk := range stream {
    fmt.Print(chunk.Content)
}
// Goroutine automatically cleaned up
```

#### Fixed Streaming Error Race Condition

**Problem**: Error channel could be written to after being closed, causing panics in high-concurrency scenarios.

**Solution**: Implemented proper channel lifecycle management with sync primitives to prevent race conditions.

#### Improved Retry Logic

The provider now includes robust retry logic with exponential backoff:

- **5 retries** for transient failures (rate limits, temporary errors)
- **Exponential backoff** with jitter (Â±30%) to prevent thundering herd
- **Max backoff** capped at 32 seconds
- **30-second timeout** for client creation

```go
// Automatically retries on transient errors
response, err := provider.Complete(ctx, request)
// Retries up to 5 times with exponential backoff on:
// - Rate limit errors (429)
// - Temporary network issues
// - Service unavailability (503)
```

#### Deterministic Outputs Fixed

**Problem**: Setting `temperature=0` didn't work as expected due to SDK parameter handling.

**Solution**: Temperature is now correctly passed to the SDK, enabling truly deterministic outputs for reproducible results.

```yaml
agents:
  - name: classifier
    role: classifier
    model: gemini-1.5-flash
    temperature: 0  # Now works correctly for deterministic outputs
    classifier_config:
      categories:
        - name: technical_issue
          description: "Technical problems or bugs"
        - name: billing_inquiry
          description: "Payment or billing questions"
```

### Security Enhancements

#### SSRF Protection for Ollama

Following our SSRF protection for external LLM providers, v0.2.0 adds the same protections to the Ollama inference service integration.

**Protected Against**:

- Private IP address access (127.0.0.0/8, 10.0.0.0/8, 172.16.0.0/12, 192.168.0.0/16)
- Metadata service endpoints (169.254.169.254)
- IPv6 private addresses
- DNS rebinding attacks

```yaml
model_services:
  - name: local-llama
    provider: huggingface
    model: meta-llama/Llama-2-7b
    runtime: ollama
    config:
      address: http://localhost:11434  # Validated for SSRF
      variant: optimized
```

**How It Works**:

```go
// URL validation before making requests
func (o *OllamaService) validateURL(address string) error {
    u, err := url.Parse(address)
    if err != nil {
        return fmt.Errorf("invalid URL: %w", err)
    }

    // Check for private IPs, metadata endpoints, etc.
    if isPrivateOrReservedIP(u.Hostname()) {
        return fmt.Errorf("access to private IP addresses not allowed")
    }

    return nil
}
```

#### Debug Logging Control

Debug logging is now controlled via the `AIXGO_DEBUG` environment variable instead of always being enabled.

```bash
# Enable debug logging
export AIXGO_DEBUG=true
go run main.go

# Production: Debug logging disabled by default
go run main.go
```

This prevents sensitive information leakage in production logs while maintaining debugging capabilities for development.

### New Features

#### Tool/Function Response Handling

The VertexAI provider now properly handles tool/function responses in the ReAct agent loop, enabling multi-step reasoning with tool calls.

```go
// ReAct loop with tool calls
agent := NewReActAgent(AgentDef{
    Name:  "assistant",
    Model: "gemini-1.5-pro",
    Tools: []Tool{
        {
            Name:        "get_weather",
            Description: "Get current weather for a location",
            Handler:     weatherHandler,
        },
    },
})

// Agent can now:
// 1. Reason about using tools
// 2. Call tools with proper parameters
// 3. Process tool responses
// 4. Continue reasoning with tool outputs
result, err := agent.Execute(ctx, &Message{
    Content: "What's the weather in San Francisco?",
})
```

#### Graceful Provider Shutdown

All providers now implement a `Close()` method for graceful cleanup of resources.

```go
provider, err := provider.NewVertexAIProvider("gemini-1.5-pro", "project-id")
if err != nil {
    log.Fatal(err)
}
defer provider.Close() // Clean up gRPC connections, clients, etc.

// Use provider...
```

### CI/CD Simplification

We've streamlined our CI/CD workflows to focus on what matters most for the open-source project:

**Removed**:

- Cloud Run deployment workflow
- Kubernetes deployment workflow

These deployment scripts remain available in the repository for users who need them, but we're no longer running them in CI.

**Enhanced**:

- Core build and test workflows
- Security scanning with SARIF uploads
- Dependency updates via Dependabot

This change reduces CI complexity while maintaining code quality and security standards.

## Upgrade Guide

### From v0.1.2 to v0.2.0

#### 1. Update Dependency

```bash
go get -u github.com/aixgo-dev/aixgo@v0.2.0
go mod tidy
```

#### 2. VertexAI Authentication Changes

If you're using the VertexAI provider, update your authentication approach:

**Before (v0.1.2)**:

```go
// Manual token management (no longer needed)
provider, err := provider.NewVertexAIProvider(model, projectID, serviceAccountJSON)
```

**After (v0.2.0)**:

```go
// Use Application Default Credentials
provider, err := provider.NewVertexAIProvider(model, projectID)
if err != nil {
    log.Fatal(err)
}
defer provider.Close() // Add cleanup
```

Set up ADC:

```bash
# Option 1: Service account key
export GOOGLE_APPLICATION_CREDENTIALS=/path/to/key.json

# Option 2: gcloud CLI
gcloud auth application-default login
```

#### 3. Add Provider Cleanup

For all providers, add `defer provider.Close()` after creation:

```go
provider, err := provider.NewOpenAIProvider("gpt-4-turbo")
if err != nil {
    log.Fatal(err)
}
defer provider.Close() // Add this

// Use provider...
```

#### 4. Review Debug Logging

If you relied on debug logs in production, enable them explicitly:

```bash
export AIXGO_DEBUG=true
```

#### 5. Test Ollama Configurations

If using Ollama, ensure your configuration doesn't use private IPs for production deployments. For local development, this works:

```yaml
model_services:
  - name: local-llama
    runtime: ollama
    config:
      address: http://localhost:11434  # OK for local development
```

For production, use public endpoints or configure SSRF allowlists if needed.

## Breaking Changes

### VertexAI Provider

- **Authentication**: The provider constructor signature changed. The `serviceAccountJSON` parameter was removed.
- **Migration**: Use Application Default Credentials (ADC) instead of passing credentials directly.

### All Providers

- **Cleanup**: All providers now have a `Close()` method. While not strictly required to call, it's recommended for graceful cleanup.

## Performance Improvements

- **Reduced Memory Usage**: Goroutine leak fix prevents unbounded memory growth in streaming scenarios
- **Faster Error Recovery**: Improved retry logic with exponential backoff reduces latency on transient failures
- **Client Reuse**: VertexAI provider now reuses HTTP clients for better connection pooling

## What's Next

Looking ahead to v0.3.0, we're planning:

- **More Provider Improvements**: Expanding SDK migrations to other providers
- **Enhanced Observability**: Better tracing for tool calls and multi-step reasoning
- **Performance Benchmarks**: Comprehensive benchmarking suite for all providers
- **Advanced Caching**: Response caching for improved latency and cost reduction

## Get Involved

We'd love your feedback on v0.2.0! Here's how you can get involved:

- **Report Issues**: [GitHub Issues](https://github.com/aixgo-dev/aixgo/issues)
- **Discussions**: [GitHub Discussions](https://github.com/orgs/aixgo-dev/discussions)
- **Contributing**: See our [Contributing Guide](https://github.com/aixgo-dev/aixgo/blob/main/docs/CONTRIBUTING.md)
- **Documentation**: [Comprehensive docs](https://github.com/aixgo-dev/aixgo/tree/main/docs)

## Resources

- **Release Notes**: [v0.2.0 on GitHub](https://github.com/aixgo-dev/aixgo/releases/tag/v0.2.0)
- **Documentation**: [docs/](https://github.com/aixgo-dev/aixgo/tree/main/docs)
- **Examples**: [examples/](https://github.com/aixgo-dev/aixgo/tree/main/examples)
- **API Reference**: [pkg.go.dev](https://pkg.go.dev/github.com/aixgo-dev/aixgo)

## Acknowledgments

Special thanks to all contributors who helped make v0.2.0 possible:

- Dependabot for automated dependency updates
- The Google Gen AI SDK team for excellent documentation
- Our community for bug reports and feature requests

---

**Download Aixgo v0.2.0 today and build production-grade AI agents with confidence.**

```bash
go get github.com/aixgo-dev/aixgo@v0.2.0
```
